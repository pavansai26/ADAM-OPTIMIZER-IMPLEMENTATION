{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ADAM OPTIMIZER IMPLEMENTATION.ipynb",
      "provenance": [],
      "private_outputs": true,
      "authorship_tag": "ABX9TyM9qDP+T0bxnoT7iXDpQZV7",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/pavansai26/ADAM-OPTIMIZER-IMPLEMENTATION/blob/master/ADAM_OPTIMIZER_IMPLEMENTATION.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w3RL2JRy8OfM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pzWsphnf9URL",
        "colab_type": "text"
      },
      "source": [
        "We compute the decaying averages of past and past squared gradients  mt  and  vt  respectively as follows:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1u1mkBxj9VOJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "20GvKPAj9Z0I",
        "colab_type": "text"
      },
      "source": [
        "$$\\begin{align}\n",
        "m_{t} &amp;= \\beta_{1} m_{t-1} + (1 - \\beta_{1}) g_{t}\\\\\n",
        "v_{t} &amp;= \\beta_{2} v_{t-1} + (1 - \\beta_{2}) g_{t}^{2}\n",
        "\\end{align}$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-pQlEF6B9sX0",
        "colab_type": "text"
      },
      "source": [
        "where $g_{t}$ is"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X94we52R90sq",
        "colab_type": "text"
      },
      "source": [
        "$$ g_{t} = \\frac{\\partial \\mathcal{L}(\\mathbf{w}_{t})}{\\partial \\mathbf{w}_{t}} $$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i-PmJwUc93ui",
        "colab_type": "text"
      },
      "source": [
        "$m_{t}$: estimate of the first moment (the mean) of the gradients\n",
        "$v_{t}$: estimate of the second moment (the uncentered variance) of the gradients"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J391RhVo94UK",
        "colab_type": "text"
      },
      "source": [
        "Bias correction"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nCEUV0F094O0",
        "colab_type": "text"
      },
      "source": [
        "$$\\begin{align}\n",
        "\\hat{m}_{t} &amp;= \\frac{m_{t}}{1 - \\beta_{1}^{t}}\\\\\n",
        "\\hat{v}_{t} &amp;= \\frac{v_{t}}{1 - \\beta_{2}^{t}}\n",
        "\\end{align}$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C2m4PMOg-zdX",
        "colab_type": "text"
      },
      "source": [
        "weight update"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7q3R7yYA-zrW",
        "colab_type": "text"
      },
      "source": [
        "$$\\mathbf{w}_{t+1} = \\mathbf{w}_{t} - \\frac{\\eta}{\\sqrt{\\hat{v}_{t}} + \\epsilon} \\hat{m}_{t}$$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "be59Kyfm9jsA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from __future__ import absolute_import\n",
        "from __future__ import division\n",
        "from __future__ import print_function\n",
        "\n",
        "import os\n",
        "import time\n",
        "\n",
        "import numpy as np"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sKl1GpZH_IWw",
        "colab_type": "text"
      },
      "source": [
        "Build a Optimizer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F6PD0mEs_C8P",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class AdamOptimizer():\n",
        "  def __init__(self, func, gradients, x_init=None, y_init=None,\n",
        "               learning_rate=0.001, beta1=0.9, beta2=0.999, epsilon=1e-8):\n",
        "    self.f = function\n",
        "    self.g = gradients\n",
        "    scale = 3.0\n",
        "    self.vars = np.zeros([2])\n",
        "    if x_init is not None:\n",
        "      self.vars[0] = x_init\n",
        "    else:\n",
        "      self.vars[0] = np.random.uniform(low=-scale, high=scale)\n",
        "    if y_init is not None:\n",
        "      self.vars[1] = y_init\n",
        "    else:\n",
        "      self.vars[1] = np.random.uniform(low=-scale, high=scale)\n",
        "    \n",
        "    print(\"x_init: {:.3f}\".format(self.vars[0]))\n",
        "    print(\"y_init: {:.3f}\".format(self.vars[1]))\n",
        "\n",
        "    self.lr = learning_rate\n",
        "    self.grads_first_moment = np.zeros([2])\n",
        "    self.grads_second_moment = np.zeros([2])\n",
        "    self.beta1 = beta1\n",
        "    self.beta2 = beta2\n",
        "    self.epsilon = epsilon\n",
        "\n",
        "     # for accumulation of loss and path (w, b)\n",
        "    self.z_history = []\n",
        "    self.x_history = []\n",
        "    self.y_history = []\n",
        "  def func(self, variables):\n",
        "    \"\"\"Beale function.\n",
        "    \n",
        "    Args:\n",
        "      variables: input data, shape: 1-rank Tensor (vector) np.array\n",
        "        x: x-dimension of inputs\n",
        "        y: y-dimension of inputs\n",
        "      \n",
        "    Returns:\n",
        "      z: Beale function value at (x, y)\n",
        "    \"\"\"\n",
        "    x, y = variables\n",
        "    z = self.f(x, y)\n",
        "    return z\n",
        "  def gradients(self, variables):\n",
        "    \"\"\"Gradient of Beale function.\n",
        "    \n",
        "    Args:\n",
        "      variables: input data, shape: 1-rank Tensor (vector) np.array\n",
        "        x: x-dimension of inputs\n",
        "        y: y-dimension of inputs\n",
        "      \n",
        "    Returns:\n",
        "      grads: [dx, dy], shape: 1-rank Tensor (vector) np.array\n",
        "        dx: gradient of Beale function with respect to x-dimension of inputs\n",
        "        dy: gradient of Beale function with respect to y-dimension of inputs\n",
        "    \"\"\"\n",
        "    x, y = variables\n",
        "    grads = self.g(x, y)\n",
        "    return grads\n",
        "  def weights_update(self, grads, time):\n",
        "    \"\"\"Weights update using Adam.\n",
        "    \n",
        "      g1 = beta1 * g1 + (1 - beta1) * grads\n",
        "      g2 = beta2 * g2 + (1 - beta2) * g2\n",
        "      g1_unbiased = g1 / (1 - beta1**time)\n",
        "      g2_unbiased = g2 / (1 - beta2**time)\n",
        "      w = w - lr * g1_unbiased / (sqrt(g2_unbiased) + epsilon)\n",
        "    \"\"\"\n",
        "    self.grads_first_moment = self.beta1 * self.grads_first_moment + \\\n",
        "                              (1. - self.beta1) * grads\n",
        "    self.grads_second_moment = self.beta2 * self.grads_second_moment + \\\n",
        "                              (1. - self.beta2) * grads**2\n",
        "    \n",
        "    self.grads_first_moment_unbiased = self.grads_first_moment / (1. - self.beta1**time)\n",
        "    self.grads_second_moment_unbiased = self.grads_second_moment / (1. - self.beta2**time)\n",
        "    \n",
        "    self.vars = self.vars - self.lr * self.grads_first_moment_unbiased /(np.sqrt(self.grads_second_moment_unbiased) + self.epsilon)\n",
        "  \n",
        "  def history_update(self, z, x, y):\n",
        "    \"\"\"Accumulate all interesting variables\n",
        "    \"\"\"\n",
        "    self.z_history.append(z)\n",
        "    self.x_history.append(x)\n",
        "    self.y_history.append(y)\n",
        "  def train(self, max_steps):\n",
        "    self.z_history = []\n",
        "    self.x_history = []\n",
        "    self.y_history = []\n",
        "    pre_z = 0.0\n",
        "    print(\"steps: {}  z: {:.6f}  x: {:.5f}  y: {:.5f}\".format(0, self.func(self.vars), self.x, self.y))\n",
        "    \n",
        "    file = open('adam.txt', 'w')\n",
        "    file.write(\"{:.5f}  {:.5f}\\n\".format(self.x, self.y))\n",
        "    \n",
        "    for step in range(max_steps):\n",
        "      self.z = self.func(self.vars)\n",
        "      self.history_update(self.z, self.x, self.y)\n",
        "\n",
        "      self.grads = self.gradients(self.vars)\n",
        "      self.weights_update(self.grads, step+1)\n",
        "      file.write(\"{:.5f}  {:.5f}\\n\".format(self.x, self.y))\n",
        "      \n",
        "      if (step+1) % 100 == 0:\n",
        "        print(\"steps: {}  z: {:.6f}  x: {:.5f}  y: {:.5f}  dx: {:.5f}  dy: {:.5f}\".format(step+1, self.func(self.vars), self.x, self.y, self.dx, self.dy))\n",
        "        \n",
        "      if np.abs(pre_z - self.z) < 1e-7:\n",
        "        print(\"Enough convergence\")\n",
        "        print(\"steps: {}  z: {:.6f}  x: {:.5f}  y: {:.5f}\".format(step+1, self.func(self.vars), self.x, self.y))\n",
        "        self.z = self.func(self.vars)\n",
        "        self.history_update(self.z, self.x, self.y)\n",
        "        break\n",
        "        \n",
        "      pre_z = self.z\n",
        "    file.close()\n",
        "\n",
        "    self.x_history = np.array(self.x_history)\n",
        "    self.y_history = np.array(self.y_history)\n",
        "    self.path = np.concatenate((np.expand_dims(self.x_history, 1), np.expand_dims(self.y_history, 1)), axis=1).T\n",
        "  @property\n",
        "  def x(self):\n",
        "    return self.vars[0]\n",
        "  \n",
        "  @property\n",
        "  def y(self):\n",
        "    return self.vars[1]\n",
        "  \n",
        "  @property\n",
        "  def dx(self):\n",
        "    return self.grads[0]\n",
        "  \n",
        "  @property\n",
        "  def dy(self):\n",
        "    return self.grads[1]\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-Ox020xG_Tnb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}